---
title: "Classification Prediction"
author: "Gustavo Monteiro"
date: "November 18, 2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(caret)
library(rattle)
library(tidyverse)
library(dataPreparation)
library(MLmetrics)
library(GGally)
library(ggmosaic)
```


> The database that will be used in this study consists of data on the votes that candidates for the Federal Chamber of Deputies received in the years 2006 and 2010 (source: http://www.tse.jus.br), as well as information on campaigning, party, schooling, ... of them.

## Loading data.
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

> Let's explore classes balance of data attributes

```{r}
train %>%
  glimpse()
```

```{r}
train %>%
  map_df(function(x) sum(is.na(x))) %>%
  gather(feature, num_nulls) %>%
  arrange(desc(num_nulls))
```

```{r}
train %>%
  ggplot(aes(situacao,recursos_proprios)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_partido_politico)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_outros_candidatos.comites)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_fisicas)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_juridicas)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             quantidade_doacoes)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             quantidade_doadores)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             media_receita)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
  ggplot(aes(situacao,
             total_receita)) +
  geom_boxplot() + 
  coord_flip()
```

```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(grau, situacao),
                   fill=grau)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(estado_civil, situacao),
                   fill=estado_civil)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```



```{r}
encoding <- build_encoding(dataSet = train,
                           cols = c("sexo"),
                           verbose = F)

train <- one_hot_encoder(dataSet = train,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)

encoding <- build_encoding(dataSet = test,
                           cols = c("sexo"),
                           verbose = F)

test <- one_hot_encoder(dataSet = test,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)
```


```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}

lambda <- expand.grid(.cp = seq(from=0, to=0.02, by=0.005))
```


```{r}
## Removing variables that was unnecessary or has a minim
train <- train %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf)

test <- test %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo, -uf)

```


```{r}
cctrl <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot")

tree <- train(situacao ~ .,
              train,
              method="rpart",
              metric = "F1",
              tuneGrid = lambda,
              preProc = c("center", "scale"),
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"))
```

```{r}
tree
```

```{r}
tree$finalModel
```

```{r}
# plot the model
plot(tree$finalModel, uniform=TRUE,
     main="Classification Tree")
text(tree$finalModel, all=TRUE, cex=.8)
```

```{r}
fancyRpartPlot(tree$finalModel)
```

```{r}
ggplot(tree)
```



```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L2_dual"),
                       epsilon = c(0.001, 0.01) )

cctrl2 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 3)

logistic <- train(situacao ~ ., train, 
                             method = "regLogistic", 
                             trControl = cctrl2,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = rlGrid)
```

```{r}
logistic
```

```{r}
logistic$finalModel
```

```{r}
ggplot(logistic)
```


```{r}
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

knn <- train(situacao ~ ., train,
        metric = "F1",
        method = "knn",
        tuneGrid = neighborsGrid,
        trControl = cctrl2)
```


```{r}
knn$bestTune
```

```{r}
ggplot(knn)
```



```{r warning=FALSE, message=FALSE}
grid <- expand.grid(mfinal = (1:3)*3, maxdepth = c(1, 3),
                    coeflearn = c("Breiman", "Freund", "Zhu"))

seeds <- vector(mode = "list", length = nrow(training) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl2 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 5)
adaboost <- train(situacao ~ ., train, 
                             method = "AdaBoost.M1", 
                             trControl = cctrl,
                             tuneGrid = grid,
                             metric = "F1", 
                             preProc = c("center", "scale"))
```

```{r}
adaboost
```

```{r}
ggplot(adaboost)
```


```{r}
seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl3 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 5, seeds = seeds)

gbm <- train(situacao ~ ., train, 
                             method = "gbm", 
                             trControl = cctrl3,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                                       shrinkage = c(.12, .13),
                                       n.trees = c(55, 56, 57, 58, 59, 60),
                                       n.minobsinnode = c(2,3)),
                             verbose = FALSE,
                            distribution = "adaboost")
gbm$bestTune
```


```{r}
ggplot(gbm)
```


```{r}
prediction <- predict(gbm, test)
data_out <- data.frame(ID = test$sequencial_candidato, Predicted = prediction) 
data_out$ID <-as.character(data_out$ID)  
data_out %>% write_csv(path = "response.csv") 
prediction
```
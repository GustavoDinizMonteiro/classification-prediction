---
title: "Classification Prediction"
author: "Gustavo Monteiro"
date: "November 18, 2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(caret)
library(rattle)
library(tidyverse)
library(dataPreparation)
library(MLmetrics)
library(GGally)
library(ggmosaic)
```


> The database that will be used in this study consists of data on the votes that candidates for the Federal Chamber of Deputies received in the years 2006 and 2010 (source: http://www.tse.jus.br), as well as information on campaigning, party, schooling, ... of them.

## Loading data.
```{r}
set.seed(1)

data <- read.csv("train.csv")
data$id <- 1:nrow(data)

data %>% 
  sample_frac(.7) -> train

test <- read.csv("test.csv")
dplyr::anti_join(data, train, by = 'id') -> validation
```

> Let's explore the data features

```{r}
train %>%
  glimpse()
```

> following, we will explore classes balance of data attributes, that is an important point to prevent overfiting in prediction models.

Feature by feature, we will show balance by target variable, which in this study is `situaÃ§ao`, whether a deputy was elected or not

### recursos_proprios
```{r}
train %>%
  ggplot(aes(situacao,recursos_proprios)) +
  geom_boxplot()
```

### recursos_de_partido_politico
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_partido_politico)) +
  geom_boxplot()
```

unbalanced, high parties recourse is more common in elected

### recursos_de_outros_candidatos.comites
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_outros_candidatos.comites)) +
  geom_boxplot()
```

the same case of `recursos_de_partido_politico`

### recursos_de_pessoas_fisicas
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_fisicas)) +
  geom_boxplot() 
```

unbalanced to a range of lower values for non-elected and high for elected

### recursos_de_pessoas_juridicas
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_juridicas)) +
  geom_boxplot()
```

unbalanced in a range of values

### quantidade_doacoes
```{r}
train %>%
  ggplot(aes(situacao,
             quantidade_doacoes)) +
  geom_boxplot()
```

unbalanced to a very high amount of donations

### quantidade_doadores
```{r}
train %>%
  ggplot(aes(situacao,
             quantidade_doadores)) +
  geom_boxplot()
```

the same situation of `quantidade_doacoes`

### media_receita
```{r}
train %>%
  ggplot(aes(situacao,
             media_receita)) +
  geom_boxplot()
```

cases of a very high value only for an unelected

### total_receita
```{r}
train %>%
  ggplot(aes(situacao,
             total_receita)) +
  geom_boxplot()
```

ranges of values that are only present in non-elected

### sexo
```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

### grau
```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(grau, situacao),
                   fill=grau)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```


### estado_civil
```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(estado_civil, situacao),
                   fill=estado_civil)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

> we can see that there is rather an imbalance in some variables, especially of values that are present only between elected or non-elected, this can cause problems of overfiting, and among the possible strategies for the correction of this problem will be used a hybrid in this study , undersampling more common values and resampling less common values.


The next step will build an encoding to transform `sexo` feature in a factor variable.
```{r}
encoding <- build_encoding(dataSet = train,
                           cols = c("sexo"),
                           verbose = F)

train <- one_hot_encoder(dataSet = train,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)

encoding <- build_encoding(dataSet = validation,
                           cols = c("sexo"),
                           verbose = F)

validation <- one_hot_encoder(dataSet = validation,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)

encoding <- build_encoding(dataSet = test,
                           cols = c("sexo"),
                           verbose = F)

test <- one_hot_encoder(dataSet = test,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)
```


Creating a personal function to evaluente F1 metric of a model.
```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}

F_Measure <- function(expected, predicted, ...) {
  data.frame(expected=expected,
             prediction=predicted) %>%
      mutate(fn = ifelse(expected == "eleito" &
                         prediction == "nao_eleito",1,0),
             fp = ifelse(expected == "nao_eleito" &
                         prediction == "eleito",1,0)) -> result
}
```

And we will remove all variables that has no or minimum variance beacuse them is uncessary for creating models.
```{r}
## Removing variables that was unnecessary or has a minim
train <- train %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf)

validation <- validation %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf)

test <- test %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo, -uf)

```

> Finally, we can start to create the five models that we will test here, begining by decision three.

## Dessision Three
```{r}
cctrl <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot", sampling = "smote")
lambda <- expand.grid(.cp = seq(from=0, to=0.02, by=0.005))

tree <- train(situacao ~ .,
              train,
              method="rpart",
              metric = "F1",
              tuneGrid = lambda,
              preProc = c("center", "scale"),
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"))
```

Resultant model
```{r}
varImp(tree)
```

and the best tunning for him
```{r}
tree$finalModel
```

Let's see the final three

```{r}
# plot the model
plot(tree$finalModel, uniform=TRUE,
     main="Classification Tree")
text(tree$finalModel, all=TRUE, cex=.8)
```

and the same but more pretty

```{r}
fancyRpartPlot(tree$finalModel)
```

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=tree,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(fn = sum(fn) / nrow(validation), fp = sum(fp) / nrow(validation)))
```

and now the F1 performance graph of this model for all hyper params tested
```{r}
ggplot(tree)
```


## Logistic regression
```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L2_dual"),
                       epsilon = c(0.001, 0.01) )

cctrl2 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 5, sampling = "smote")

logistic <- train(situacao ~ ., train, 
                             method = "regLogistic", 
                             trControl = cctrl2,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = rlGrid)
```

Resultant model
```{r}
varImp(logistic)
```

and the best tunning for him
```{r}
logistic$finalModel
```

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=logistic,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(fn = sum(fn) / nrow(validation), fp = sum(fp) / nrow(validation)))
```


and now the F1 performance graph of this model for all hyper params tested
```{r}
ggplot(logistic)
```


## KNN
```{r}
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

knn <- train(situacao ~ ., train,
        metric = "F1",
        method = "knn",
        tuneGrid = neighborsGrid,
        trControl = cctrl2)
```

Resultant model
```{r}
varImp(knn)
```

and the best tunning for him
```{r}
knn$bestTune
```

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=knn,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(fn = sum(fn) / nrow(validation), fp = sum(fp) / nrow(validation)))
```

and now the F1 performance graph of this model for all hyper params tested
```{r}
ggplot(knn)
```

## AdaBoost
```{r warning=FALSE, message=FALSE}
grid <- expand.grid(mfinal = (1:3)*3, maxdepth = c(1, 3),
                    coeflearn = c("Breiman", "Freund", "Zhu"))

seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl2 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 10, sampling = "smote")
adaboost <- train(situacao ~ ., train, 
                             method = "AdaBoost.M1", 
                             trControl = cctrl,
                             tuneGrid = grid,
                             metric = "F1", 
                             preProc = c("center", "scale"))
```

Resultant model
```{r}
varImp(adaboost)
```

and the best tunning for him
```{r}
adaboost$finalModel
```

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=adaboost,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(fn = sum(fn) / nrow(validation), fp = sum(fp) / nrow(validation)))
```

and now the F1 performance graph of this model for all hyper params tested
```{r}
ggplot(adaboost)
```


> In all models, and shown more explicitly in the decision tree, parameters for spending money in the field seem to be the most important, and show what really makes a difference for a candidate to be elected or not.

## Following, we will test and big famous model, the gradient boosting.

```{r}
seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl3 <- trainControl(
  summaryFunction = f1,
  classProbs = TRUE,
  method = "cv",
  number = 10,
  seeds = seeds)

gbm <- train(situacao ~ ., train, 
                             method = "gbm", 
                             trControl = cctrl3,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                                       shrinkage = c(.12, .13),
                                       n.trees = c(55, 56, 57, 58, 59, 60),
                                       n.minobsinnode = c(2,3)),
                             verbose = FALSE,
                            distribution = "adaboost")
```

the best tunning for him
```{r}
gbm$bestTune
```


```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=gbm,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(fn = sum(fn) / nrow(validation), fp = sum(fp) / nrow(validation)))
```

and now the F1 performance graph of this model for all hyper params tested
```{r}
ggplot(gbm)
```


Now, lets predict results using test data for submit for keggle class competition.
```{r}
prediction <- predict(knn, test)
data_out <- data.frame(ID = test$sequencial_candidato, Predicted = prediction) 
data_out$ID <-as.character(data_out$ID)  
data_out %>% write_csv(path = "response.csv") 
prediction
```
---
title: "Classification Prediction"
author: "Gustavo Monteiro"
date: "November 18, 2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(caret)
library(rattle)
library(tidyverse)
library(dataPreparation)
library(MLmetrics)
library(GGally)
library(ggmosaic)
```


> The database that will be used in this study consists of data on the votes that candidates for the Federal Chamber of Deputies received in the years 2006 and 2010 (source: http://www.tse.jus.br), as well as information on campaigning, party, schooling, ... of them.

## Loading data.
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```


```{r}
train %>%
  glimpse()
```

```{r}
train %>%
  map_df(function(x) sum(is.na(x))) %>%
  gather(feature, num_nulls) %>%
  arrange(desc(num_nulls))
```

### Imbalance on class distribution

```{r}
train %>%
  ggplot(aes(situacao)) +
  geom_bar()
```

```{r}
train %>% 
  select(-ano,
         -sequencial_candidato,
         -nome) %>%
  select(
    quantidade_doacoes,
    quantidade_doadores,
    total_receita,
    media_receita,
    recursos_de_outros_candidatos.comites,
    recursos_de_pessoas_fisicas,
    recursos_de_pessoas_juridicas,
    recursos_proprios,
    `recursos_de_partido_politico`) %>%
  na.omit() %>%
  ggcorr(palette = "RdBu", label = TRUE,
       hjust = 0.95, label_size = 3,size = 3,
       nbreaks = 5, layout.exp = 5) +
  ggtitle("Correlation plot for employed variables")
```

```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```


```{r}
encoding <- build_encoding(dataSet = train,
                           cols = c("sexo"),
                           verbose = F)

train <- one_hot_encoder(dataSet = train,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)
```


```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}

lambda <- expand.grid(.cp = seq(from=0, to=0.02, by=0.005))
```


```{r}
## Removing variables that was unnecessary or has a minim
train <- train %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf)

```


```{r}
cctrl <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot")

tree <- train(situacao ~ .,
              train,
              method="rpart",
              metric = "F1",
              tuneGrid = lambda,
              preProc = c("center", "scale"),
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"))


ggtree <- train(situacao ~ .,
              train,
                               method = "rpart",
                               tuneGrid = tree$bestTune,
                               metric = "F1", 
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"),
                                preProc = c("center", "scale"))
```

```{r}
ggtree
```

```{r}
ggtree$finalModel
```

```{r}
# plot the model
plot(ggtree$finalModel, uniform=TRUE,
     main="Classification Tree")
text(ggtree$finalModel, all=TRUE, cex=.8)
```

```{r}
fancyRpartPlot(ggtree$finalModel)
```



```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = c(0.001,0.01) )

logistic <- train(situacao ~ ., train, 
                             method = "regLogistic", 
                             trControl = cctrl,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = rlGrid)
```

```{r}
logistic
```

```{r}
logistic$finalModel
```

```{r warning=FALSE}
seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl$seeds <- seeds

adaboost <- train(situacao ~ ., train,
                             method = "AdaBoost.M1",
                             trControl = cctrl,
                             preProc = c("center", "scale"), 
                            tuneGrid = data.frame(mfinal = 10, 
                                                     maxdepth = 1,
                                                     coeflearn = "Zhu"))
```

```{r}
adaboost
```

```{r}
adaboost$finalModel
```


---
title: "Classification Prediction"
author: "Gustavo Monteiro"
date: "November 18, 2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(caret)
library(rattle)
library(tidyverse)
library(dataPreparation)
library(MLmetrics)
library(GGally)
library(ggmosaic)
```


> The database that will be used in this study consists of data on the votes that candidates for the Federal Chamber of Deputies received in the years 2006 and 2010 (source: http://www.tse.jus.br), as well as information on campaigning, party, schooling, ... of them.

## Loading data.
```{r}
set.seed(1)

data <- read.csv("train.csv")
data$id <- 1:nrow(data)

data %>% 
  sample_frac(.9) -> train

test <- read.csv("test.csv")
anti_join(data, train, by = 'id') -> validation
```

> Let's explore the data features

```{r}
train %>%
  glimpse()
```

> following, we will explore classes balance of data attributes, that is an important point to prevent overfiting in prediction models.

Feature by feature, we will show balance by target variable, which in this study is `situaÃ§ao`, whether a deputy was elected or not, and comment if the class is umbalanced

### recursos_proprios
```{r}
train %>%
  ggplot(aes(situacao,recursos_proprios)) +
  geom_boxplot()
```

unbalanced, high ranges recourse is more common in non-elected

### recursos_de_partido_politico
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_partido_politico)) +
  geom_boxplot()
```

unbalanced, high ranges recourse is more common in elected

### recursos_de_outros_candidatos.comites
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_outros_candidatos.comites)) +
  geom_boxplot()
```

the same situation of `recursos_de_partido_politico`

### recursos_de_pessoas_fisicas
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_fisicas)) +
  geom_boxplot() 
```

unbalanced to a range of lower values for non-elected and high for elected

### recursos_de_pessoas_juridicas
```{r}
train %>%
  ggplot(aes(situacao,
             recursos_de_pessoas_juridicas)) +
  geom_boxplot()
```

unbalanced in a range of values

### quantidade_doacoes
```{r}
train %>%
  ggplot(aes(situacao,
             quantidade_doacoes)) +
  geom_boxplot()
```

unbalanced to a very high amount of donations

### quantidade_doadores
```{r}
train %>%
  ggplot(aes(situacao,
             quantidade_doadores)) +
  geom_boxplot()
```

the same situation of `quantidade_doacoes`

### media_receita
```{r}
train %>%
  ggplot(aes(situacao,
             media_receita)) +
  geom_boxplot()
```

cases of a very high value only for an unelected

### total_receita
```{r}
train %>%
  ggplot(aes(situacao,
             total_receita)) +
  geom_boxplot()
```

ranges of values that are only present in non-elected

### sexo
```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

### grau
```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(grau, situacao),
                   fill=grau)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```


### estado_civil
```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(estado_civil, situacao),
                   fill=estado_civil)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

> we can see that there is rather an imbalance in some variables, especially of values that are present only between elected or non-elected, this can cause problems of overfiting, and among the possible strategies for the correction of this problem will be used a hybrid in this study , undersampling more common values and resampling less common values.


The next step will build an encoding to transform `sexo` feature in a factor variable.
```{r}
encoding <- build_encoding(dataSet = train,
                           cols = c("sexo"),
                           verbose = F)

train <- one_hot_encoder(dataSet = train,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)

encoding <- build_encoding(dataSet = validation,
                           cols = c("sexo"),
                           verbose = F)

validation <- one_hot_encoder(dataSet = validation,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)

encoding <- build_encoding(dataSet = test,
                           cols = c("sexo"),
                           verbose = F)

test <- one_hot_encoder(dataSet = test,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)
```


## Creating a personal function to evaluente F1 metric of a model.
```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}

F_Measure <- function(expected, predicted, ...) {
  data.frame(expected=expected,
             prediction=predicted) %>%
      mutate(fn = ifelse(expected == "eleito" &
                         prediction == "nao_eleito",1,0),
             fp = ifelse(expected == "nao_eleito" &
                         prediction == "eleito",1,0)) -> result
}
```

## And we will remove all variables that has no or minimum variance beacuse them is uncessary for creating models.
```{r}
## Removing variables that was unnecessary or has a minim
train <- train %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf, -id)

validation <- validation %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf, -id)

test <- test %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo, -uf)

```

> Finally, we can start to create the five models that we will test here, begining by decision three.

# Dessision Three
```{r}
set.seed(1)

cctrl <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot", sampling = "smote")
lambda <- expand.grid(.cp = seq(from=0, to=0.02, by=0.005))

tree <- train(situacao ~ .,
              train,
              method="rpart",
              metric = "F1",
              tuneGrid = lambda,
              preProc = c("center", "scale"),
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"))
```

## Resultant model
```{r}
varImp(tree)
```

## and the best tunning for him
```{r}
tree$finalModel
```

## Let's see the final three
```{r}
# plot the model
plot(tree$finalModel, uniform=TRUE,
     main="Classification Tree")
text(tree$finalModel, all=TRUE, cex=.8)
```

## and the same but more pretty
```{r}
fancyRpartPlot(tree$finalModel)
```

## Model validation using false positive and false negative percentages of predictions

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=tree,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(false_negative = sum(fn) / nrow(validation), false_positive = sum(fp) / nrow(validation)))
```

## and now the F1 performance graph of this model for all hyper params tested
```{r}
ggplot(tree)
```

the F1 metric is medin to good, and the performance in validation predictions of false potives and false negatives together account for about 8%,
for a medium F1 were good results

# Logistic regression
```{r}
set.seed(1)
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L2_dual"),
                       epsilon = c(0.001, 0.01) )

cctrl2 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 5, sampling = "smote")

logistic <- train(situacao ~ ., train, 
                             method = "regLogistic", 
                             trControl = cctrl2,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = rlGrid)
```

## Resultant model
```{r}
varImp(logistic)
```

## and the best tunning for him

```{r}
logistic$finalModel
```

## Model validation using false positive and false negative percentages of predictions

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=logistic,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(false_negative = sum(fn) / nrow(validation), false_positive = sum(fp) / nrow(validation)))
```


## and now the F1 performance graph of this model for all hyper params tested

```{r}
ggplot(logistic)
```

the F1 metric is median, and the performance in validation predictions of false potives and false negatives together account for about 8%,
for a medium F1 were good results, with more false negatives than false positives.

# KNN
```{r}
set.seed(1)
neighborsGrid <- expand.grid(.k = seq(from=1, to=50, by=1))

knn <- train(situacao ~ ., train,
        metric = "F1",
        method = "knn",
        tuneGrid = neighborsGrid,
        trControl = cctrl2)
```

## Resultant model

```{r}
varImp(knn)
```

## and the best tunning for him

```{r}
knn$bestTune
```

## Model validation using false positive and false negative percentages of predictions

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=knn,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(false_negative = sum(fn) / nrow(validation), false_positive = sum(fp) / nrow(validation)))
```

## and now the F1 performance graph of this model for all hyper params tested

```{r}
ggplot(knn)
```

the F1 metric performance is similar to the decision tree, the same in false positives/negatives prcentage in predictions of validation set.

# AdaBoost
```{r warning=FALSE, message=FALSE}
set.seed(1)
grid <- expand.grid(mfinal = (1:3)*3, maxdepth = c(1, 2),
                    coeflearn = c("Breiman", "Freund", "Zhu"))

seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl2 <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "cv", number = 5)
adaboost <- train(situacao ~ ., train, 
                             method = "AdaBoost.M1", 
                             trControl = cctrl,
                            na.action = na.exclude,
                             tuneGrid = grid,
                             metric = "F1", 
                             preProc = c("center", "scale"))
```

## Resultant model

```{r}
varImp(adaboost)
```

## Model validation using false positive and false negative percentages of predictions

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=adaboost,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(false_negative = sum(fn) / nrow(validation), false_positive = sum(fp) / nrow(validation)))
```

## and now the F1 performance graph of this model for all hyper params tested

```{r}
ggplot(adaboost)
```

the F1 metric is median, with a courious results of validation, a minimal percentage of false negatives and more than 11% of false positives.

> In all models, and shown more explicitly in the decision tree, parameters for spending money in the field seem to be the most important, and show what really makes a difference for a candidate to be elected or not.

## Following, we will test and big famous model, the gradient boosting, as a extra model.

```{r}
seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl3 <- trainControl(
  summaryFunction = f1,
  classProbs = TRUE,
  method = "cv",
  number = 10,
  seeds = seeds)

gbm <- train(situacao ~ ., train, 
                             method = "gbm", 
                             trControl = cctrl3,
                             metric = "F1",
             na.action = na.exclude,
                             preProc = c("center", "scale"),
                             tuneGrid = expand.grid(interaction.depth = c(1, 2, 3, 4, 5),
                                       shrinkage = c(.12, .13),
                                       n.trees = c(55, 56, 57, 58, 59),
                                       n.minobsinnode = c(2,3)),
                             verbose = FALSE,
                            distribution = "adaboost")
```

## the best tunning for him

```{r}
gbm$bestTune
```

## Model validation using false positive and false negative percentages of predictions

```{r}
data.frame(validation %>%
  select(-situacao) %>%
  predict(object=gbm,.) %>%
  F_Measure(validation$situacao,.) %>% 
  summarise(false_negative = sum(fn) / nrow(validation), false_positive = sum(fp) / nrow(validation)))
```

## and now the F1 performance graph of this model for all hyper params tested

```{r}
ggplot(gbm)
```

the F1 metric is the best achieved among the models created in this study, even the performance in validation being practically the same as the other models in false positives/negatives.

## Results

The results of this study show that the models achieve a reasonable performance for metric F1, being the gradient boosting model the chosen one for the sending to competition of the Kaggle, we also saw that in the general, variables referring to financial investment and support of companies were the features that in all models showed to have the greatest relevance

 > Generation predictions for test dataframe to submit to Kaggle, to related [competition](https://www.kaggle.com/c/ufcg-cdp-20182).

```{r}
prediction <- predict(gbm, test)
data_out <- data.frame(ID = test$sequencial_candidato, Predicted = prediction) 
data_out$ID <-as.character(data_out$ID)  
data_out %>% write_csv(path = "response.csv")
```
---
title: "Classification Prediction"
author: "Gustavo Monteiro"
date: "November 18, 2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(caret)
library(rattle)
library(tidyverse)
library(dataPreparation)
```


> The database that will be used in this study consists of data on the votes that candidates for the Federal Chamber of Deputies received in the years 2006 and 2010 (source: http://www.tse.jus.br), as well as information on campaigning, party, schooling, ... of them.

## Loading data.
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```


```{r}
# train <- train %>% 
#   mutate(uf = as.factor(uf),
#          nome = as.factor(nome),
#          sexo = as.factor(sexo),
#          grau = as.factor(grau),
#          nome = as.factor(nome),
#          cargo = as.factor(cargo),
#          partido = as.factor(partido),
#          situacao = as.factor(situacao),
#          ocupacao = as.factor(ocupacao),
#          estado_civil = as.factor(estado_civil),
#          sequencial_candidato = as.numeric(sequencial_candidato))
# 
# encoding <- build_encoding(dataSet = train,
#                            cols = c("uf","sexo","grau",
#                                     "partido","estado_civil"),
#                            verbose = F)
# 
# train <- one_hot_encoder(dataSet = train,
#                            encoding = encoding,
#                            drop = TRUE,
#                            verbose = F)
```


```{r}
## Removing categoric variables.
train <- train %>% select(-nome, -uf, -estado_civil, 
                          -partido, -ocupacao,-ano, 
                          -cargo,-grau,-sexo, 
                          -sequencial_candidato)

```


```{r}
tree <- train(situacao ~ .,
              train,
              method="rpart", 
              trControl = trainControl(method = "boot"))

cctrl3 <- trainControl(method = "none", classProbs = TRUE, summaryFunction = twoClassSummary)

ggtree <- train(situacao ~ .,
              train,
                               method = "rpart", 
                               trControl = cctrl3,
                               tuneGrid = tree$bestTune,
                               metric = "ROC", 
                                preProc = c("center", "scale"))
```

```{r}
ggtree
```

```{r}
ggtree$finalModel
```

```{r}
# plot the model
plot(ggtree$finalModel, uniform=TRUE,
     main="Classification Tree")
text(ggtree$finalModel, all=TRUE, cex=.8)
```

```{r}
fancyRpartPlot(ggtree$finalModel)
```


```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = c(0.001,0.01) )

logistic <- train(situacao ~ ., train, 
                             method = "regLogistic", 
                             trControl = trainControl(method = "boot", classProbs = TRUE),
                             metric = "ROC", 
                             preProc = c("center", "scale"),
                             tuneGrid = rlGrid)
```

```{r}
logistic
```

```{r}
logistic$finalModel
```

```{r}
seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl3 <- trainControl(method = "none",
                       classProbs = TRUE, 
                       summaryFunction = twoClassSummary,
                       seeds = seeds)

adaboost <- train(situacao ~ ., train,
                             method = "AdaBoost.M1",
                             trControl = cctrl3,
                             preProc = c("center", "scale"), 
                            tuneGrid = data.frame(mfinal = 10, 
                                                     maxdepth = 1,
                                                     coeflearn = "Zhu"))
```

```{r}
adaboost
```

```{r}
adaboost$finalModel
```


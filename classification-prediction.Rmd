---
title: "Classification Prediction"
author: "Gustavo Monteiro"
date: "November 18, 2018"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, echo=FALSE, message=FALSE}
library(caret)
library(rattle)
library(tidyverse)
library(dataPreparation)
library(MLmetrics)
library(GGally)
library(ggmosaic)
```


> The database that will be used in this study consists of data on the votes that candidates for the Federal Chamber of Deputies received in the years 2006 and 2010 (source: http://www.tse.jus.br), as well as information on campaigning, party, schooling, ... of them.

## Loading data.
```{r}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```


```{r}
train %>%
  glimpse()
```

```{r}
train %>%
  map_df(function(x) sum(is.na(x))) %>%
  gather(feature, num_nulls) %>%
  arrange(desc(num_nulls))
```

### Imbalance on class distribution

```{r}
train %>%
  ggplot(aes(situacao)) +
  geom_bar()
```

```{r}
train %>% 
  select(-ano,
         -sequencial_candidato,
         -nome) %>%
  select(
    quantidade_doacoes,
    quantidade_doadores,
    total_receita,
    media_receita,
    recursos_de_outros_candidatos.comites,
    recursos_de_pessoas_fisicas,
    recursos_de_pessoas_juridicas,
    recursos_proprios,
    `recursos_de_partido_politico`) %>%
  na.omit() %>%
  ggcorr(palette = "RdBu", label = TRUE,
       hjust = 0.95, label_size = 3,size = 3,
       nbreaks = 5, layout.exp = 5) +
  ggtitle("Correlation plot for employed variables")
```

```{r}
train %>%
ggplot() +
   geom_mosaic(aes(x = product(sexo, situacao),
                   fill=sexo)) +
   theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```


```{r}
encoding <- build_encoding(dataSet = train,
                           cols = c("sexo"),
                           verbose = F)

train <- one_hot_encoder(dataSet = train,
                           encoding = encoding,
                           drop = TRUE,
                           verbose = F)
```


```{r}
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  c(F1 = f1_val)
}

lambda <- expand.grid(.cp = seq(from=0, to=0.02, by=0.005))
```


```{r}
## Removing variables that was unnecessary or has a minim
train <- train %>% select(-nome,-ano, -ocupacao, -partido,
                          -estado_civil, -grau, -cargo,
                          -sequencial_candidato, -uf)

```


```{r}
cctrl <- trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot")

tree <- train(situacao ~ .,
              train,
              method="rpart",
              metric = "F1",
              tuneGrid = lambda,
              preProc = c("center", "scale"),
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"))


ggtree <- train(situacao ~ .,
              train,
                               method = "rpart",
                               tuneGrid = tree$bestTune,
                               metric = "F1", 
              trControl = trainControl(summaryFunction = f1, classProbs = TRUE, method = "boot"),
                                preProc = c("center", "scale"))
```

```{r}
ggtree
```

```{r}
ggtree$finalModel
```

```{r}
# plot the model
plot(ggtree$finalModel, uniform=TRUE,
     main="Classification Tree")
text(ggtree$finalModel, all=TRUE, cex=.8)
```

```{r}
fancyRpartPlot(ggtree$finalModel)
```

```{r}
ggplot(tree)
```



```{r}
rlGrid <- expand.grid( cost = c(200,2,0.02),
                       loss = c("L1", "L2_dual", "L2_primal"),
                       epsilon = seq(from=0, to=0.02, by=0.005))

cctrl$method = "cv"

logistic <- train(situacao ~ ., train, 
                             method = "regLogistic", 
                             trControl = cctrl,
                             metric = "F1", 
                             preProc = c("center", "scale"),
                             tuneGrid = rlGrid)
```

```{r}
logistic
```

```{r}
logistic$finalModel
```

```{r}
ggplot(logistic)
```


```{r warning=FALSE, message=FALSE}
seeds <- vector(mode = "list", length = nrow(train) + 1)
seeds <- lapply(seeds, function(x) 1:20)

cctrl$seeds <- seeds
cctrl$method <- "boot"

adaboost <- train(situacao ~ ., train,
                             method = "AdaBoost.M1",
                             trControl = cctrl,
                             preProc = c("center", "scale"), 
                            tuneGrid = data.frame(mfinal = 10, 
                                                     maxdepth = 1,
                                                     coeflearn = "Zhu"))
```

```{r}
adaboost
```

```{r}
ggplot(adaboost)
```

